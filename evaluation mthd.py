# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nB4Csn5xbHeW81_SUiIybwISUs1t4PGh
"""

def rolling_forward_backtest(X, Y, C):
    """
    Expanding Window Rolling-Forward Backtest (정석)
    매 anchor t에 대해:
        - Train: 0 ~ t
        - Test: t -> t+H 예측
        - 모델 매번 재학습
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    N = len(X)
    mse_list = []

    for anchor in range(L, N - H):
        print(f"[Rolling-Forward] Anchor = {anchor}/{N}")

        # ----------------------------
        # 1) Expanding Window Train Split
        # ----------------------------
        X_train = X[:anchor]
        Y_train = Y[:anchor]
        C_train = C[:anchor]

        # Test one step (anchor)
        X_test = X[anchor:anchor+1]
        Y_test = Y[anchor:anchor+1]
        C_test = C[anchor:anchor+1]

        # ----------------------------
        # 2) 모델 재학습
        # ----------------------------
        model = train_model(
            X_train,
            Y_train,
            C_train,
            epochs=50  # 너무 느리면 줄여도 됨
        )

        # ----------------------------
        # 3) Forecast (True Forecast Mode)
        # ----------------------------
        with torch.no_grad():
            x = torch.tensor(X_test, dtype=torch.float32).to(device)
            c = torch.tensor(C_test, dtype=torch.float32).to(device)

            mean, _, _ = model(
                x, c,
                y=None,
                use_prior_sampling_if_no_y=True
            )

            y_pred = mean.cpu().numpy()

        # ----------------------------
        # 4) MSE 계산
        # ----------------------------
        mse = np.mean((y_pred - Y_test)**2)
        mse_list.append(mse)

    # 전체 anchor 평균 MSE
    return np.mean(mse_list)

def rolling_backtest(model_path, X, Y, C):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 모델 리로드
    out_dim = X.shape[-1]

    encoder = Encoder(
        x_dim=out_dim, c_dim=COND_DIM, h_dim=HIDDEN, z_dim=LATENT_DIM
    ).to(device)

    decoder = Decoder(
        latent_dim=LATENT_DIM, cond_dim=COND_DIM, out_dim=out_dim,
        hidden=HIDDEN, H=H, poly_order=POLY_ORDER, n_fourier=N_FOURIER
    ).to(device)

    prior = ConditionalPrior(
        cond_dim=COND_DIM, latent_dim=LATENT_DIM, hidden=HIDDEN
    ).to(device)

    model = TimeVAE(
        encoder=encoder, decoder=decoder, prior=prior,
        latent_dim=LATENT_DIM, beta=BETA
    ).to(device)

    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    mses = []

    for t in range(len(X)):
        x = torch.tensor(X[t:t+1], dtype=torch.float32).to(device)
        c = torch.tensor(C[t:t+1], dtype=torch.float32).to(device)
        y_true = Y[t:t+1]   # numpy

        # forecast mode
        with torch.no_grad():
            y_pred, _, _ = model(x, c, y=None, use_prior_sampling_if_no_y=True)
            y_pred = y_pred.cpu().numpy()

        mse_t = np.mean((y_pred - y_true)**2)
        mses.append(mse_t)

    return np.mean(mses)